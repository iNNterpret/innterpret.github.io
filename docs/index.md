# Representation of speech, articulatory dynamics, prosody and language in layers. What do the models know?

## Motivation

Neural architectures have accelerated the growth of speech applications in a wide variety of fields not just restricted to traditional automatic speech recognition (ASR) and text-to-speech (TTS) synthesis - these include, deducing and increasing the interpretability of neural networks, articulatory-acoustic-mapping, and neural language models, and extensive and precise modeling of speech prosody. This session will be dedicated to provide talks from a wide-variety of such perspectives to bring together advances that highlight how computational and linguistic approaches converge in both providing technological and scholarly solutions to classical problems in speech -- acoustics, articulation, prosody, and technology.

The non-linear dynamics between speech articulation and acoustics is both elegance and a source of confounds for building viable applications and knowledge representations. We will begin this session with an overview of how neural architectures have been used for learning phonological patterns, following that we will look into how speech prosody and phonation can help build representations that are holistic. We will also investigate how the non-linearity between articulation and acoustics can be examined to offer insights into the acoustic-articulatory mapping and how this mapping can be used for prediction in several domains. Our special invited lecture will shed light on how Generative Adversarial Networks are used in forming generalizations akin to phonological representations.

### Speakers

- What do RNNs learn when they learn vowel harmony? 
	- 9:00-9:25 am: Frédéric Mailhot
- Kristin Yu

